{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# word2vec\n",
    "\n",
    "Ten notatnik ma na celu przedstawienie sposobu tworzenia i wykorzystania reprezentacji werktorowych na przykładzie algorytmu word2vec. W trakcie zadania najpierw stworzymy prostą reprezentację wektorową, a następnie spróbujemy wczytać gotowy model nauczony na dużym korpusie tekstowym.\n",
    "\n",
    "Po wykonaniu tego zadania powinieneś:\n",
    "+ wiedzieć na czym polega word2vec,\n",
    "+ potrafić stworzyć word2vec na własnych danych,\n",
    "+ potrafić wykorzystać word2vec do:\n",
    "\t+ znalezienia podobnych słów,\n",
    "\t+ wyszukiwania słów na zasadzie \"reguły trzech\", \n",
    "\t+ wykrywania niepasujących słów,\n",
    "\t+ do tworzenia wektora cech nadającego się do klasyfikacji,\n",
    "+ wczytać i wykorzystać gotowy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prosty model\n",
    "\n",
    "Najpierw wczytamy odpowiednie biblioteki i stworzymy mały zbiór treningowy na podstawie znanej piosenki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging, re, nltk\n",
    "import pandas as pd\n",
    "\n",
    "RE_SPACES = re.compile(\"\\s+\")\n",
    "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
    "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
    "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "song = \"\"\"Gdzie strumyk płynie z wolna,\n",
    "Rozsiewa zioła maj,\n",
    "Stokrotka rosła polna,\n",
    "A nad nią szumiał gaj,\n",
    "Stokrotka rosła polna,\n",
    "A nad nią szumiał gaj,\n",
    "Zielony gaj.\n",
    "\n",
    "W tym gaju tak ponuro,\n",
    "Że aż przeraża mnie,\n",
    "Ptaszęta za wysoko,\n",
    "A mnie samotnej źle,\n",
    "Ptaszęta za wysoko,\n",
    "A mnie samotnej źle,\n",
    "samotnej źle.\n",
    "\n",
    "Wtem harcerz idzie z wolna.\n",
    "„Stokrotko, witam cię,\n",
    "Twój urok mnie zachwyca,\n",
    "Czy chcesz być mą, czy nie?”\n",
    "\"Twój urok mnie zachwyca,\n",
    "Czy chcesz być mą, czy nie?\n",
    "Czy nie, czy nie?\n",
    "\n",
    "Stokrotka się zgodziła\n",
    "I poszli w ciemny las,\n",
    "A harcerz taki gapa\n",
    "że aż w pokrzywy wlazł,\n",
    "A harcerz taki gapa\n",
    "że aż w pokrzywy wlazł,\n",
    "w pokrzywy wlazł.\n",
    "\n",
    "A ona, ona, ona,\n",
    "Cóż biedna robić ma,\n",
    "Nad gapą pochylona\n",
    "I śmieje się: ha, ha,\n",
    "Nad gapą pochylona\n",
    "I śmieje: się ha, ha,\n",
    "ha, ha, ha, ha.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 1: Podziel piosenkę na wersy, a wersy tokenizuj spacjami. W efekcie powinieneś stworzyć listę list i przypisać ją do zmiennej `sentences`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Gdzie', 'strumyk', 'płynie', 'z', 'wolna,'], ['Rozsiewa', 'zioła', 'maj,'], ['Stokrotka', 'rosła', 'polna,'], ['A', 'nad', 'nią', 'szumiał', 'gaj,'], ['Stokrotka', 'rosła', 'polna,'], ['A', 'nad', 'nią', 'szumiał', 'gaj,'], ['Zielony', 'gaj.'], [''], ['W', 'tym', 'gaju', 'tak', 'ponuro,'], ['Że', 'aż', 'przeraża', 'mnie,'], ['Ptaszęta', 'za', 'wysoko,'], ['A', 'mnie', 'samotnej', 'źle,'], ['Ptaszęta', 'za', 'wysoko,'], ['A', 'mnie', 'samotnej', 'źle,'], ['samotnej', 'źle.'], [''], ['Wtem', 'harcerz', 'idzie', 'z', 'wolna.'], ['„Stokrotko,', 'witam', 'cię,'], ['Twój', 'urok', 'mnie', 'zachwyca,'], ['Czy', 'chcesz', 'być', 'mą,', 'czy', 'nie?”'], ['\"Twój', 'urok', 'mnie', 'zachwyca,'], ['Czy', 'chcesz', 'być', 'mą,', 'czy', 'nie?'], ['Czy', 'nie,', 'czy', 'nie?'], [''], ['Stokrotka', 'się', 'zgodziła'], ['I', 'poszli', 'w', 'ciemny', 'las,'], ['A', 'harcerz', 'taki', 'gapa'], ['że', 'aż', 'w', 'pokrzywy', 'wlazł,'], ['A', 'harcerz', 'taki', 'gapa'], ['że', 'aż', 'w', 'pokrzywy', 'wlazł,'], ['w', 'pokrzywy', 'wlazł.'], [''], ['A', 'ona,', 'ona,', 'ona,'], ['Cóż', 'biedna', 'robić', 'ma,'], ['Nad', 'gapą', 'pochylona'], ['I', 'śmieje', 'się:', 'ha,', 'ha,'], ['Nad', 'gapą', 'pochylona'], ['I', 'śmieje:', 'się', 'ha,', 'ha,'], ['ha,', 'ha,', 'ha,', 'ha.']]\n"
     ]
    }
   ],
   "source": [
    "sentences = [RE_SPACES.split(paragraph) for paragraph in song.split('\\n')]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając tekst podzielony na zdania a zdania na tokeny, możemy nauczyć model word2vec.\n",
    "\n",
    "**Zad. 2: Naucz model word2vec. [Sprawdź](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) za co odpowiedzialne są parametry `min_count` i `iter`. Jakie inne parametry mogą być przydatne?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* min_count - ignoruje wszystkie słowa, które występują rzadziej niż min_count\n",
    "* iter - liczba iteracji po korpusie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-17 22:31:08,975 : INFO : collecting all words and their counts\n",
      "2021-01-17 22:31:08,977 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-01-17 22:31:08,979 : INFO : collected 80 word types from a corpus of 144 raw words and 39 sentences\n",
      "2021-01-17 22:31:08,980 : INFO : Loading a fresh vocabulary\n",
      "2021-01-17 22:31:08,982 : INFO : effective_min_count=1 retains 80 unique words (100% of original 80, drops 0)\n",
      "2021-01-17 22:31:08,985 : INFO : effective_min_count=1 leaves 144 word corpus (100% of original 144, drops 0)\n",
      "2021-01-17 22:31:08,988 : INFO : deleting the raw counts dictionary of 80 items\n",
      "2021-01-17 22:31:08,989 : INFO : sample=0.001 downsamples 80 most-common words\n",
      "2021-01-17 22:31:08,992 : INFO : downsampling leaves estimated 50 word corpus (35.2% of prior 144)\n",
      "2021-01-17 22:31:08,993 : INFO : estimated required memory for 80 words and 100 dimensions: 104000 bytes\n",
      "2021-01-17 22:31:08,995 : INFO : resetting layer weights\n",
      "2021-01-17 22:31:09,037 : INFO : training model with 3 workers on 80 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-01-17 22:31:09,041 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:09,043 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:09,046 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:09,047 : INFO : EPOCH - 1 : training on 144 raw words (48 effective words) took 0.0s, 7190 effective words/s\n",
      "2021-01-17 22:31:09,053 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:09,056 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:09,056 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:09,058 : INFO : EPOCH - 2 : training on 144 raw words (53 effective words) took 0.0s, 12799 effective words/s\n",
      "2021-01-17 22:31:09,062 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:09,064 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:09,068 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:09,070 : INFO : EPOCH - 3 : training on 144 raw words (47 effective words) took 0.0s, 5415 effective words/s\n",
      "2021-01-17 22:31:09,075 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:09,077 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:09,078 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:09,080 : INFO : EPOCH - 4 : training on 144 raw words (62 effective words) took 0.0s, 11773 effective words/s\n",
      "2021-01-17 22:31:09,086 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:09,088 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:09,090 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:09,092 : INFO : EPOCH - 5 : training on 144 raw words (51 effective words) took 0.0s, 7932 effective words/s\n",
      "2021-01-17 22:31:09,104 : INFO : training on a 720 raw words (261 effective words) took 0.1s, 3977 effective words/s\n",
      "2021-01-17 22:31:09,115 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2021-01-17 22:31:09,122 : INFO : precomputing L2-norms of word weight vectors\n",
      "2021-01-17 22:31:09,126 : WARNING : vectors for words {'las', 'gaj'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=80, size=100, alpha=0.025)\n",
      "<gensim.models.word2vec.Word2VecVocab object at 0x7f3898bc7f28>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmiki/Repo/EMD/venv/lib/python3.6/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'harcerz'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, iter=5, min_count=1)\n",
    "print(model)\n",
    "print(model.vocabulary)\n",
    "\n",
    "model.wv.doesnt_match(\"las harcerz gaj zioła\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model jest niezwykle mały i niezbyt praktyczny, ale pozwolił pokazać podstawę uczenia word2vec. Przy większych korpusach tekstowych wczytywanie do pamięci wielkich tablic nie byłoby najlepszym pomysłem. Na szczęście implementacja word2vec w gensim potrafi przetwarzać dane przyrostowo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przetwarzanie strumieniowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast wczytywać wszystkie dokumenty naraz można robić to partiami, bo sieci neuronowe (w tym word2vec) potrafią douczać się przyrostowo. Do douczania przyrostowego świetnie nada się pythonowy iterator lub generator. Jeśli nie kojarzysz na czym polega działanie iteratorów i generatorów, zobacz jak [wyjaśnia to Radim Rehurek](https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/).\n",
    "\n",
    "Zasymulujmy zdania/wersy/tweety przechowywane w osobnych plikach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open, os\n",
    "\n",
    "if not os.path.exists('./data/'):\n",
    "    os.makedirs('./data/')\n",
    "\n",
    "filenames = ['./data/f' + str(i) +'.txt' for i in range(39)]\n",
    "\n",
    "if sentences is not None:\n",
    "    for i, fname in enumerate(filenames):\n",
    "        with smart_open.smart_open(fname, 'w') as fout:\n",
    "            for line in sentences[i]:\n",
    "                fout.write(line + ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 3: Mając powyższy zbiór dokumentów tekstowych, stwórz metodę która będzie \"leniwie\" iterowała przez zasymulowany zbiór danych. Podczas iterowania usuń znaki interpunkcyjne i zmień wszystkie litery na małe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-17 22:31:19,457 : INFO : collecting all words and their counts\n",
      "2021-01-17 22:31:19,461 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-01-17 22:31:19,475 : INFO : collected 66 word types from a corpus of 183 raw words and 39 sentences\n",
      "2021-01-17 22:31:19,477 : INFO : Loading a fresh vocabulary\n",
      "2021-01-17 22:31:19,478 : INFO : effective_min_count=1 retains 66 unique words (100% of original 66, drops 0)\n",
      "2021-01-17 22:31:19,479 : INFO : effective_min_count=1 leaves 183 word corpus (100% of original 183, drops 0)\n",
      "2021-01-17 22:31:19,484 : INFO : deleting the raw counts dictionary of 66 items\n",
      "2021-01-17 22:31:19,485 : INFO : sample=0.001 downsamples 66 most-common words\n",
      "2021-01-17 22:31:19,486 : INFO : downsampling leaves estimated 53 word corpus (29.5% of prior 183)\n",
      "2021-01-17 22:31:19,490 : INFO : estimated required memory for 66 words and 100 dimensions: 85800 bytes\n",
      "2021-01-17 22:31:19,493 : INFO : resetting layer weights\n",
      "2021-01-17 22:31:19,525 : INFO : training model with 3 workers on 66 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-01-17 22:31:19,541 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:19,542 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:19,543 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:19,544 : INFO : EPOCH - 1 : training on 183 raw words (51 effective words) took 0.0s, 3155 effective words/s\n",
      "2021-01-17 22:31:19,566 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:19,567 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:19,568 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:19,570 : INFO : EPOCH - 2 : training on 183 raw words (57 effective words) took 0.0s, 2994 effective words/s\n",
      "2021-01-17 22:31:19,590 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:19,591 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:19,592 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:19,594 : INFO : EPOCH - 3 : training on 183 raw words (46 effective words) took 0.0s, 2277 effective words/s\n",
      "2021-01-17 22:31:19,618 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:19,620 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:19,621 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:19,623 : INFO : EPOCH - 4 : training on 183 raw words (41 effective words) took 0.0s, 1856 effective words/s\n",
      "2021-01-17 22:31:19,641 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:19,642 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:19,643 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:19,644 : INFO : EPOCH - 5 : training on 183 raw words (53 effective words) took 0.0s, 3298 effective words/s\n",
      "2021-01-17 22:31:19,662 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:19,663 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:19,666 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:19,667 : INFO : EPOCH - 6 : training on 183 raw words (55 effective words) took 0.0s, 3152 effective words/s\n",
      "2021-01-17 22:31:19,686 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:19,687 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:19,688 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:19,689 : INFO : EPOCH - 7 : training on 183 raw words (54 effective words) took 0.0s, 2980 effective words/s\n",
      "2021-01-17 22:31:19,709 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:19,710 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:19,711 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:19,712 : INFO : EPOCH - 8 : training on 183 raw words (63 effective words) took 0.0s, 3482 effective words/s\n",
      "2021-01-17 22:31:19,728 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:19,729 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:19,730 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:19,735 : INFO : EPOCH - 9 : training on 183 raw words (59 effective words) took 0.0s, 3451 effective words/s\n",
      "2021-01-17 22:31:19,755 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:19,756 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:19,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:19,758 : INFO : EPOCH - 10 : training on 183 raw words (48 effective words) took 0.0s, 3019 effective words/s\n",
      "2021-01-17 22:31:19,759 : INFO : training on a 1830 raw words (527 effective words) took 0.2s, 2263 effective words/s\n",
      "2021-01-17 22:31:19,760 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2021-01-17 22:31:19,761 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=66, size=100, alpha=0.025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'las'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            if fname.endswith('.txt'):\n",
    "                for line in open(os.path.join(self.dirname, fname)):\n",
    "                    yield RE_SPACES.split(''.join([char for char in line.lower() if char not in punctuation]))\n",
    "\n",
    "# Do odkomentowania:\n",
    "sentences = MySentences('./data/')\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
    "print(model)\n",
    "\n",
    "model.wv.doesnt_match(\"las harcerz gaj zioła\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trochę więcej danych i przykłady zastosowań\n",
    "\n",
    "Jak wspomniano wcześniej powyższa piosenka jest zbyt krótka by stworzyć przekonujący model podobieństwa między słowami. Przejdziemy teraz na język angielski i wykorzystamy korpus dołączony do biblioteki `gensim`. Ten korpus nie jest jeszcze duży, więc wyniki nie będą rewelacyjne. Potrzeba > 500 tys. słów, żeby oczekiwać rozsądnych wyników dla ogólnych zapytań, ale przy specjalistycznych zastosowaniach korpusy niekoniecznie muszą być takie duże.\n",
    "\n",
    "**Zad. 4: Korzystając ze zdobytej wiedzy na temat iteratorów, uzupełnij poniższy kod.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
    "lee_train_file = test_data_dir + 'lee_background.cor'\n",
    "\n",
    "class MyText(object):\n",
    "    def __iter__(self):\n",
    "        for line in open(lee_train_file):\n",
    "            # Załóż, że każda linia to dokument, zmień litery na małe,\n",
    "            # usuń podstawowe znaki interpunkcyjne i podziel według białych znaków\n",
    "            yield RE_SPACES.split(''.join([char for char in line.lower() if char not in punctuation]))\n",
    "\n",
    "sentences = MyText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 5: Naucz model word2vec o rozmiarze 200, przez 100 epok, usuwając słowa występującerzadziej niż 5 razy. Wynik przypisz do zmiennej `model`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-17 22:31:26,330 : INFO : collecting all words and their counts\n",
      "2021-01-17 22:31:26,334 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-01-17 22:31:26,427 : INFO : collected 7586 word types from a corpus of 60146 raw words and 300 sentences\n",
      "2021-01-17 22:31:26,428 : INFO : Loading a fresh vocabulary\n",
      "2021-01-17 22:31:26,440 : INFO : effective_min_count=5 retains 1791 unique words (23% of original 7586, drops 5795)\n",
      "2021-01-17 22:31:26,441 : INFO : effective_min_count=5 leaves 50552 word corpus (84% of original 60146, drops 9594)\n",
      "2021-01-17 22:31:26,454 : INFO : deleting the raw counts dictionary of 7586 items\n",
      "2021-01-17 22:31:26,456 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2021-01-17 22:31:26,458 : INFO : downsampling leaves estimated 36364 word corpus (71.9% of prior 50552)\n",
      "2021-01-17 22:31:26,466 : INFO : estimated required memory for 1791 words and 200 dimensions: 3761100 bytes\n",
      "2021-01-17 22:31:26,467 : INFO : resetting layer weights\n",
      "2021-01-17 22:31:26,948 : INFO : training model with 3 workers on 1791 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-01-17 22:31:27,051 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:27,052 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:27,064 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:27,065 : INFO : EPOCH - 1 : training on 60146 raw words (36261 effective words) took 0.1s, 316909 effective words/s\n",
      "2021-01-17 22:31:27,174 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:27,176 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:27,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:27,190 : INFO : EPOCH - 2 : training on 60146 raw words (36323 effective words) took 0.1s, 297567 effective words/s\n",
      "2021-01-17 22:31:27,309 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:27,310 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:27,321 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:27,324 : INFO : EPOCH - 3 : training on 60146 raw words (36418 effective words) took 0.1s, 292525 effective words/s\n",
      "2021-01-17 22:31:27,430 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:27,431 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:27,443 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:27,444 : INFO : EPOCH - 4 : training on 60146 raw words (36371 effective words) took 0.1s, 309502 effective words/s\n",
      "2021-01-17 22:31:27,549 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:27,552 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:27,565 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:27,567 : INFO : EPOCH - 5 : training on 60146 raw words (36413 effective words) took 0.1s, 307983 effective words/s\n",
      "2021-01-17 22:31:27,666 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:27,668 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:27,680 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:27,681 : INFO : EPOCH - 6 : training on 60146 raw words (36329 effective words) took 0.1s, 341381 effective words/s\n",
      "2021-01-17 22:31:27,788 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:27,790 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:27,804 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:27,806 : INFO : EPOCH - 7 : training on 60146 raw words (36349 effective words) took 0.1s, 367975 effective words/s\n",
      "2021-01-17 22:31:27,909 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:27,910 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:27,921 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:27,923 : INFO : EPOCH - 8 : training on 60146 raw words (36370 effective words) took 0.1s, 330621 effective words/s\n",
      "2021-01-17 22:31:28,027 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:28,029 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:28,039 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:28,041 : INFO : EPOCH - 9 : training on 60146 raw words (36290 effective words) took 0.1s, 327477 effective words/s\n",
      "2021-01-17 22:31:28,149 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:28,150 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:28,164 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:28,165 : INFO : EPOCH - 10 : training on 60146 raw words (36491 effective words) took 0.1s, 307719 effective words/s\n",
      "2021-01-17 22:31:28,270 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:28,271 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:28,285 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:28,286 : INFO : EPOCH - 11 : training on 60146 raw words (36330 effective words) took 0.1s, 317302 effective words/s\n",
      "2021-01-17 22:31:28,406 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:28,408 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:28,418 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:28,419 : INFO : EPOCH - 12 : training on 60146 raw words (36426 effective words) took 0.1s, 284368 effective words/s\n",
      "2021-01-17 22:31:28,525 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:28,526 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:28,537 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:28,537 : INFO : EPOCH - 13 : training on 60146 raw words (36339 effective words) took 0.1s, 334319 effective words/s\n",
      "2021-01-17 22:31:28,646 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:28,647 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:28,661 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:28,662 : INFO : EPOCH - 14 : training on 60146 raw words (36438 effective words) took 0.1s, 308435 effective words/s\n",
      "2021-01-17 22:31:28,760 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:28,761 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:28,774 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:28,776 : INFO : EPOCH - 15 : training on 60146 raw words (36400 effective words) took 0.1s, 333724 effective words/s\n",
      "2021-01-17 22:31:28,878 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:28,879 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:28,892 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:28,894 : INFO : EPOCH - 16 : training on 60146 raw words (36326 effective words) took 0.1s, 325064 effective words/s\n",
      "2021-01-17 22:31:28,996 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:28,998 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:29,011 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:29,012 : INFO : EPOCH - 17 : training on 60146 raw words (36341 effective words) took 0.1s, 328296 effective words/s\n",
      "2021-01-17 22:31:29,116 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:29,117 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-17 22:31:29,126 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:29,128 : INFO : EPOCH - 18 : training on 60146 raw words (36428 effective words) took 0.1s, 335562 effective words/s\n",
      "2021-01-17 22:31:29,230 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:29,234 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:29,249 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:29,250 : INFO : EPOCH - 19 : training on 60146 raw words (36413 effective words) took 0.1s, 304045 effective words/s\n",
      "2021-01-17 22:31:29,357 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:29,358 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:29,371 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:29,372 : INFO : EPOCH - 20 : training on 60146 raw words (36338 effective words) took 0.1s, 310100 effective words/s\n",
      "2021-01-17 22:31:29,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:29,469 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:29,484 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:29,486 : INFO : EPOCH - 21 : training on 60146 raw words (36288 effective words) took 0.1s, 330951 effective words/s\n",
      "2021-01-17 22:31:29,575 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:29,577 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:29,592 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:29,593 : INFO : EPOCH - 22 : training on 60146 raw words (36434 effective words) took 0.1s, 350704 effective words/s\n",
      "2021-01-17 22:31:29,698 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:29,700 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:29,712 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:29,714 : INFO : EPOCH - 23 : training on 60146 raw words (36409 effective words) took 0.1s, 307615 effective words/s\n",
      "2021-01-17 22:31:29,825 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:29,827 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:29,840 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:29,841 : INFO : EPOCH - 24 : training on 60146 raw words (36263 effective words) took 0.1s, 295208 effective words/s\n",
      "2021-01-17 22:31:29,933 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:29,936 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:29,945 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:29,946 : INFO : EPOCH - 25 : training on 60146 raw words (36395 effective words) took 0.1s, 357367 effective words/s\n",
      "2021-01-17 22:31:30,057 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:30,058 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:30,071 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:30,073 : INFO : EPOCH - 26 : training on 60146 raw words (36438 effective words) took 0.1s, 294505 effective words/s\n",
      "2021-01-17 22:31:30,188 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:30,190 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:30,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:30,204 : INFO : EPOCH - 27 : training on 60146 raw words (36361 effective words) took 0.1s, 284876 effective words/s\n",
      "2021-01-17 22:31:30,307 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:30,308 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:30,320 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:30,322 : INFO : EPOCH - 28 : training on 60146 raw words (36338 effective words) took 0.1s, 319870 effective words/s\n",
      "2021-01-17 22:31:30,436 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:30,437 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:30,448 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:30,449 : INFO : EPOCH - 29 : training on 60146 raw words (36291 effective words) took 0.1s, 293969 effective words/s\n",
      "2021-01-17 22:31:30,549 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:30,550 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:30,561 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:30,562 : INFO : EPOCH - 30 : training on 60146 raw words (36318 effective words) took 0.1s, 330277 effective words/s\n",
      "2021-01-17 22:31:30,676 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:30,677 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:30,690 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:30,691 : INFO : EPOCH - 31 : training on 60146 raw words (36274 effective words) took 0.1s, 290150 effective words/s\n",
      "2021-01-17 22:31:30,789 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:30,791 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:30,802 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:30,803 : INFO : EPOCH - 32 : training on 60146 raw words (36389 effective words) took 0.1s, 334952 effective words/s\n",
      "2021-01-17 22:31:30,925 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:30,926 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:30,939 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:30,940 : INFO : EPOCH - 33 : training on 60146 raw words (36341 effective words) took 0.1s, 276131 effective words/s\n",
      "2021-01-17 22:31:31,066 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:31,067 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:31,079 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:31,080 : INFO : EPOCH - 34 : training on 60146 raw words (36299 effective words) took 0.1s, 263774 effective words/s\n",
      "2021-01-17 22:31:31,187 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:31,189 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:31,200 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:31,201 : INFO : EPOCH - 35 : training on 60146 raw words (36405 effective words) took 0.1s, 309774 effective words/s\n",
      "2021-01-17 22:31:31,327 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:31,328 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:31,344 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:31,345 : INFO : EPOCH - 36 : training on 60146 raw words (36345 effective words) took 0.1s, 261531 effective words/s\n",
      "2021-01-17 22:31:31,475 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:31,477 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:31,489 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:31,493 : INFO : EPOCH - 37 : training on 60146 raw words (36347 effective words) took 0.1s, 255153 effective words/s\n",
      "2021-01-17 22:31:31,648 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:31,651 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:31,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-17 22:31:31,665 : INFO : EPOCH - 38 : training on 60146 raw words (36399 effective words) took 0.2s, 226082 effective words/s\n",
      "2021-01-17 22:31:31,771 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:31,773 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:31,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:31,787 : INFO : EPOCH - 39 : training on 60146 raw words (36419 effective words) took 0.1s, 304950 effective words/s\n",
      "2021-01-17 22:31:31,898 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:31,899 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:31,911 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:31,912 : INFO : EPOCH - 40 : training on 60146 raw words (36369 effective words) took 0.1s, 302203 effective words/s\n",
      "2021-01-17 22:31:32,021 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:32,023 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:32,035 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:32,037 : INFO : EPOCH - 41 : training on 60146 raw words (36345 effective words) took 0.1s, 302893 effective words/s\n",
      "2021-01-17 22:31:32,140 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:32,141 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:32,158 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:32,159 : INFO : EPOCH - 42 : training on 60146 raw words (36376 effective words) took 0.1s, 310095 effective words/s\n",
      "2021-01-17 22:31:32,270 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:32,272 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:32,286 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:32,287 : INFO : EPOCH - 43 : training on 60146 raw words (36332 effective words) took 0.1s, 293774 effective words/s\n",
      "2021-01-17 22:31:32,399 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:32,400 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:32,413 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:32,415 : INFO : EPOCH - 44 : training on 60146 raw words (36423 effective words) took 0.1s, 291124 effective words/s\n",
      "2021-01-17 22:31:32,523 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:32,525 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:32,539 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:32,540 : INFO : EPOCH - 45 : training on 60146 raw words (36438 effective words) took 0.1s, 318392 effective words/s\n",
      "2021-01-17 22:31:32,651 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:32,654 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:32,665 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:32,666 : INFO : EPOCH - 46 : training on 60146 raw words (36322 effective words) took 0.1s, 296877 effective words/s\n",
      "2021-01-17 22:31:32,771 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:32,772 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:32,784 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:32,786 : INFO : EPOCH - 47 : training on 60146 raw words (36265 effective words) took 0.1s, 319227 effective words/s\n",
      "2021-01-17 22:31:32,891 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:32,893 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:32,907 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:32,908 : INFO : EPOCH - 48 : training on 60146 raw words (36408 effective words) took 0.1s, 319085 effective words/s\n",
      "2021-01-17 22:31:33,024 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:33,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:33,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:33,037 : INFO : EPOCH - 49 : training on 60146 raw words (36413 effective words) took 0.1s, 291066 effective words/s\n",
      "2021-01-17 22:31:33,153 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:33,156 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:33,168 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:33,169 : INFO : EPOCH - 50 : training on 60146 raw words (36429 effective words) took 0.1s, 286176 effective words/s\n",
      "2021-01-17 22:31:33,357 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:33,358 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:33,393 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:33,396 : INFO : EPOCH - 51 : training on 60146 raw words (36323 effective words) took 0.2s, 163549 effective words/s\n",
      "2021-01-17 22:31:33,521 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:33,522 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:33,535 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:33,536 : INFO : EPOCH - 52 : training on 60146 raw words (36312 effective words) took 0.1s, 263741 effective words/s\n",
      "2021-01-17 22:31:33,649 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:33,651 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:33,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:33,664 : INFO : EPOCH - 53 : training on 60146 raw words (36344 effective words) took 0.1s, 302011 effective words/s\n",
      "2021-01-17 22:31:33,790 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:33,791 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:33,803 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:33,805 : INFO : EPOCH - 54 : training on 60146 raw words (36388 effective words) took 0.1s, 264246 effective words/s\n",
      "2021-01-17 22:31:33,911 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:33,914 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:33,924 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:33,926 : INFO : EPOCH - 55 : training on 60146 raw words (36416 effective words) took 0.1s, 318097 effective words/s\n",
      "2021-01-17 22:31:34,035 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:34,036 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:34,050 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:34,050 : INFO : EPOCH - 56 : training on 60146 raw words (36463 effective words) took 0.1s, 300748 effective words/s\n",
      "2021-01-17 22:31:34,159 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:34,160 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:34,179 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:34,180 : INFO : EPOCH - 57 : training on 60146 raw words (36387 effective words) took 0.1s, 288897 effective words/s\n",
      "2021-01-17 22:31:34,282 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:34,284 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:34,296 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:34,297 : INFO : EPOCH - 58 : training on 60146 raw words (36377 effective words) took 0.1s, 321806 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-17 22:31:34,405 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:34,406 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:34,419 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:34,420 : INFO : EPOCH - 59 : training on 60146 raw words (36209 effective words) took 0.1s, 301657 effective words/s\n",
      "2021-01-17 22:31:34,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:34,520 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:34,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:34,532 : INFO : EPOCH - 60 : training on 60146 raw words (36318 effective words) took 0.1s, 334384 effective words/s\n",
      "2021-01-17 22:31:34,649 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:34,650 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:34,664 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:34,665 : INFO : EPOCH - 61 : training on 60146 raw words (36380 effective words) took 0.1s, 281505 effective words/s\n",
      "2021-01-17 22:31:34,769 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:34,771 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:34,780 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:34,781 : INFO : EPOCH - 62 : training on 60146 raw words (36359 effective words) took 0.1s, 331554 effective words/s\n",
      "2021-01-17 22:31:34,881 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:34,883 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:34,895 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:34,896 : INFO : EPOCH - 63 : training on 60146 raw words (36268 effective words) took 0.1s, 337970 effective words/s\n",
      "2021-01-17 22:31:35,010 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:35,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:35,025 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:35,026 : INFO : EPOCH - 64 : training on 60146 raw words (36356 effective words) took 0.1s, 287830 effective words/s\n",
      "2021-01-17 22:31:35,130 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:35,131 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:35,143 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:35,144 : INFO : EPOCH - 65 : training on 60146 raw words (36388 effective words) took 0.1s, 318947 effective words/s\n",
      "2021-01-17 22:31:35,254 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:35,256 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:35,266 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:35,266 : INFO : EPOCH - 66 : training on 60146 raw words (36392 effective words) took 0.1s, 308568 effective words/s\n",
      "2021-01-17 22:31:35,374 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:35,376 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:35,388 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:35,389 : INFO : EPOCH - 67 : training on 60146 raw words (36353 effective words) took 0.1s, 304852 effective words/s\n",
      "2021-01-17 22:31:35,505 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:35,506 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:35,518 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:35,519 : INFO : EPOCH - 68 : training on 60146 raw words (36360 effective words) took 0.1s, 288850 effective words/s\n",
      "2021-01-17 22:31:35,625 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:35,626 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:35,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:35,639 : INFO : EPOCH - 69 : training on 60146 raw words (36391 effective words) took 0.1s, 320743 effective words/s\n",
      "2021-01-17 22:31:35,758 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:35,761 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:35,776 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:35,778 : INFO : EPOCH - 70 : training on 60146 raw words (36333 effective words) took 0.1s, 267141 effective words/s\n",
      "2021-01-17 22:31:35,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:35,894 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:35,909 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:35,910 : INFO : EPOCH - 71 : training on 60146 raw words (36506 effective words) took 0.1s, 283349 effective words/s\n",
      "2021-01-17 22:31:36,011 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:36,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:36,027 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:36,028 : INFO : EPOCH - 72 : training on 60146 raw words (36259 effective words) took 0.1s, 320021 effective words/s\n",
      "2021-01-17 22:31:36,141 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:36,142 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:36,154 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:36,155 : INFO : EPOCH - 73 : training on 60146 raw words (36348 effective words) took 0.1s, 294931 effective words/s\n",
      "2021-01-17 22:31:36,268 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:36,269 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:36,280 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:36,280 : INFO : EPOCH - 74 : training on 60146 raw words (36387 effective words) took 0.1s, 300721 effective words/s\n",
      "2021-01-17 22:31:36,382 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:36,383 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:36,392 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:36,394 : INFO : EPOCH - 75 : training on 60146 raw words (36259 effective words) took 0.1s, 336395 effective words/s\n",
      "2021-01-17 22:31:36,494 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:36,497 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:36,508 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:36,509 : INFO : EPOCH - 76 : training on 60146 raw words (36395 effective words) took 0.1s, 327084 effective words/s\n",
      "2021-01-17 22:31:36,603 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:36,605 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:36,616 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:36,617 : INFO : EPOCH - 77 : training on 60146 raw words (36436 effective words) took 0.1s, 350655 effective words/s\n",
      "2021-01-17 22:31:36,727 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:36,729 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:36,738 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:36,739 : INFO : EPOCH - 78 : training on 60146 raw words (36313 effective words) took 0.1s, 307612 effective words/s\n",
      "2021-01-17 22:31:36,838 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-17 22:31:36,839 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:36,850 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:36,851 : INFO : EPOCH - 79 : training on 60146 raw words (36356 effective words) took 0.1s, 333376 effective words/s\n",
      "2021-01-17 22:31:36,970 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:36,971 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:36,983 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:36,984 : INFO : EPOCH - 80 : training on 60146 raw words (36283 effective words) took 0.1s, 286632 effective words/s\n",
      "2021-01-17 22:31:37,083 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:37,084 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:37,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:37,095 : INFO : EPOCH - 81 : training on 60146 raw words (36332 effective words) took 0.1s, 338735 effective words/s\n",
      "2021-01-17 22:31:37,194 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:37,196 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:37,206 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:37,207 : INFO : EPOCH - 82 : training on 60146 raw words (36331 effective words) took 0.1s, 335784 effective words/s\n",
      "2021-01-17 22:31:37,309 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:37,310 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:37,321 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:37,322 : INFO : EPOCH - 83 : training on 60146 raw words (36293 effective words) took 0.1s, 333389 effective words/s\n",
      "2021-01-17 22:31:37,421 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:37,422 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:37,436 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:37,437 : INFO : EPOCH - 84 : training on 60146 raw words (36397 effective words) took 0.1s, 329405 effective words/s\n",
      "2021-01-17 22:31:37,539 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:37,541 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:37,554 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:37,555 : INFO : EPOCH - 85 : training on 60146 raw words (36329 effective words) took 0.1s, 319312 effective words/s\n",
      "2021-01-17 22:31:37,650 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:37,651 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:37,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:37,664 : INFO : EPOCH - 86 : training on 60146 raw words (36284 effective words) took 0.1s, 343745 effective words/s\n",
      "2021-01-17 22:31:37,765 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:37,766 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:37,777 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:37,779 : INFO : EPOCH - 87 : training on 60146 raw words (36336 effective words) took 0.1s, 325199 effective words/s\n",
      "2021-01-17 22:31:37,881 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:37,883 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:37,894 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:37,895 : INFO : EPOCH - 88 : training on 60146 raw words (36376 effective words) took 0.1s, 328757 effective words/s\n",
      "2021-01-17 22:31:37,996 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:37,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:38,009 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:38,010 : INFO : EPOCH - 89 : training on 60146 raw words (36362 effective words) took 0.1s, 326324 effective words/s\n",
      "2021-01-17 22:31:38,111 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:38,114 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:38,122 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:38,124 : INFO : EPOCH - 90 : training on 60146 raw words (36402 effective words) took 0.1s, 329769 effective words/s\n",
      "2021-01-17 22:31:38,213 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:38,215 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:38,227 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:38,228 : INFO : EPOCH - 91 : training on 60146 raw words (36371 effective words) took 0.1s, 363484 effective words/s\n",
      "2021-01-17 22:31:38,335 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:38,336 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:38,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:38,347 : INFO : EPOCH - 92 : training on 60146 raw words (36293 effective words) took 0.1s, 314042 effective words/s\n",
      "2021-01-17 22:31:38,456 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:38,457 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:38,467 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:38,469 : INFO : EPOCH - 93 : training on 60146 raw words (36417 effective words) took 0.1s, 306518 effective words/s\n",
      "2021-01-17 22:31:38,626 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:38,627 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:38,640 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:38,642 : INFO : EPOCH - 94 : training on 60146 raw words (36311 effective words) took 0.2s, 215571 effective words/s\n",
      "2021-01-17 22:31:38,778 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:38,779 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:38,797 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:38,798 : INFO : EPOCH - 95 : training on 60146 raw words (36348 effective words) took 0.1s, 260797 effective words/s\n",
      "2021-01-17 22:31:38,919 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:38,921 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:38,932 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:38,933 : INFO : EPOCH - 96 : training on 60146 raw words (36358 effective words) took 0.1s, 277859 effective words/s\n",
      "2021-01-17 22:31:39,034 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:39,036 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:39,048 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:39,048 : INFO : EPOCH - 97 : training on 60146 raw words (36230 effective words) took 0.1s, 322130 effective words/s\n",
      "2021-01-17 22:31:39,152 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:39,155 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:39,163 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:39,163 : INFO : EPOCH - 98 : training on 60146 raw words (36328 effective words) took 0.1s, 341641 effective words/s\n",
      "2021-01-17 22:31:39,263 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:39,265 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-17 22:31:39,277 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:39,278 : INFO : EPOCH - 99 : training on 60146 raw words (36349 effective words) took 0.1s, 324545 effective words/s\n",
      "2021-01-17 22:31:39,378 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-17 22:31:39,379 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-17 22:31:39,392 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-17 22:31:39,393 : INFO : EPOCH - 100 : training on 60146 raw words (36318 effective words) took 0.1s, 335556 effective words/s\n",
      "2021-01-17 22:31:39,394 : INFO : training on a 6014600 raw words (3635691 effective words) took 12.4s, 292153 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=1791, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, size=200, iter=100, min_count=5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 6: Odkomentuj poniższe linie i zobacz jak można wykorzystać uzyskany model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-17 22:31:42,240 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rights', 0.5049401521682739)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['human', 'crime'], negative=['party'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-17 22:31:43,488 : WARNING : vectors for words {'lunch', 'cat', 'input'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"input is lunch he sentence cat\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053768706\n",
      "0.19906503\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('human', 'tree'))\n",
    "print(model.wv.similarity('crime', 'murder'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uwagi dodatkowe:**\n",
    "+ uczenie modelu można zrównoleglić, ale trzeba doinstalować [Cythona](http://cython.org/)\n",
    "+ wytrenowany model można łatwo zapisać do pliku za pomocą: `model.save(path)`\n",
    "+ równie łatwo można go później wczytać: `model = gensim.models.Word2Vec.load(path)`\n",
    "+ ponieważ uczenie jest przyrostowe, można łatwo rozszerzyć istniejący słownik i douczyć model na nowych zdaniach:\n",
    "```\n",
    "model = gensim.models.Word2Vec.load(path)\n",
    "more_sentences = [['Advanced', 'users', 'can', 'load', 'a', 'model', 'and', 'continue', \n",
    "                  'training', 'it', 'with', 'more', 'sentences']]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystanie gotowego modelu do klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Póki co sami trenowaliśmy word2vec i to na niedużych zbiorach danych. Na szczęście są już gotowe modele (przynajmniej dla języka angielskiego) nauczone na miliardach dokumentów i zawierające miliony słów. Przydatna lista takich modeli (wraz z kodem tworzącym usługę sieciową wykorzystującą model...) pod adresem: https://github.com/3Top/word2vec-api.\n",
    "\n",
    "**Zad. 7: Pobierz korpus Google News i zapisz pobrany plik do folderu data. Następnie wykonaj poniższy kod. Ta operacja zajmie jakieś 3-4 minuty i zużyje ok. 4 GB RAMU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-17 22:31:48,301 : INFO : loading projection weights from data/GoogleNews-vectors-negative300.bin.gz\n",
      "2021-01-17 22:33:15,726 : INFO : loaded (3000000, 300) matrix from data/GoogleNews-vectors-negative300.bin.gz\n",
      "2021-01-17 22:33:15,728 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 20.3 s, total: 1min 22s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 8: Zobacz jak działa model nauczony na tak dużym korpusie.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76640123\n",
      "0.32413527\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('woman', 'man'))\n",
    "print(wv.similarity('woman', 'cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, super. Mam świetny model, mogę nim podpowiadać słowa, wynajdować niepasujące elementy, uzupełniać zdania, znajdować synonimy, itd. Ale czy da się to jakoś wykorzystać do klasyfikacji? word2vec ma wektor na każde słowo - jak z tego zrobić wektor na ciąg słów?\n",
    "\n",
    "**Odpowiedź: można uśrednić znaczenie słów w dokuemncie poprzez zsumowanie wektorów wszystkich słów.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        return np.zeros(wv.layer_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bardzo szybko spróbujemy zastosować to podejście do predykcji gatunku filmu na podstawie jego opisu. Poniżej kod wczytujący ciekawy zbiór danych oraz pokazujący jakie ma klasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-17 22:34:20,610 : INFO : Generating new fontManager, this may take some time...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  movieId                                               plot  \\\n",
      "0           0        1  A little boy named Andy loves to be in his roo...   \n",
      "1           1        2  When two kids find and play a magical board ga...   \n",
      "2           2        3  Things don't seem to change much in Wabasha Co...   \n",
      "3           3        6  Hunters and their prey--Neil and his professio...   \n",
      "4           4        7  An ugly duckling having undergone a remarkable...   \n",
      "\n",
      "         tag  \n",
      "0  animation  \n",
      "1    fantasy  \n",
      "2     comedy  \n",
      "3     action  \n",
      "4    romance  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv('data/tagged_plots_movielens.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "print(df.head())\n",
    "df.tag.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Szybko dzielimy dane na zbiór uczący i testowy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizujemy dane i wyliczamy reprezentację wektorową (za pomocą zsumowanych wektorów słów wrod2vec):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bartmiki/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "test_tokenized = test_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
    "train_tokenized = train_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uczymy i testujemy klasyfikator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmiki/Repo/EMD/venv/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "\n",
    "logreg = logreg.fit(X_train_word_average, train_data['tag'])\n",
    "predicted = logreg.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patrzymy jak nam poszło:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trafność klasyfikacji 0.5390946502057613\n",
      "Macierz pomyłek\n",
      " [[23  2 10  0  1  6]\n",
      " [ 3 10  8  3  4  3]\n",
      " [ 2  5 55  2 18  4]\n",
      " [ 3  4  4  4  0  1]\n",
      " [ 4  0 12  1 16  2]\n",
      " [ 5  0  3  0  2 23]]\n"
     ]
    }
   ],
   "source": [
    "print('Trafność klasyfikacji %s' % accuracy_score(test_data.tag, predicted))\n",
    "cm = confusion_matrix(test_data.tag, predicted)\n",
    "print('Macierz pomyłek\\n %s' % cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak na brak porządnego przetwarzania wstępnego, nie jest to zły wynik. Mam nadzieję, że ten przykład pokazał jak można wykorzystać word2vec do tworzenia atrybutów dla problemów klasyfikacyjnych."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
